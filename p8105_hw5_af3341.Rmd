---
title: "p8105_hw5_af3341"
author: "Alana Ferris"
date: "2022-11-09"
output: github_document
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

library(rvest)

set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1 

## Creating a Dataframe

```{r}
long_study_df = list.files(path = "./data")

study_df = tibble(files = long_study_df)

read_files = 
  function(filename) {
  participant_data = 
  read_csv(file = paste0("./data/", filename)) %>%
  mutate(study_arm = filename)
}

study_data =
  map_df(study_df, read_files)
```

## Tidying the Dataframe

```{r}
tidy_df = 
  study_data %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value",
    names_prefix = "week_"
  ) %>% 
  separate(study_arm, into = c("study_arm", "subject_id"), sep = "_", convert = TRUE) %>% 
  mutate(
    subject_id = str_replace(subject_id, ".csv", "")) %>%
  group_by(study_arm, subject_id) %>% 
  mutate(
    unique_id = paste0(study_arm, subject_id)
  ) %>% 
  select(study_arm, subject_id, unique_id, week, value)

```

## Spaghetti Plot 

```{r, warning = FALSE, message = FALSE}
tidy_df %>% 
  ggplot(aes(x = week, y = value, group = unique_id, color = study_arm)) + 
  geom_line() +
  labs(
    x = "Week of Observation",
    y = "Value",
    title = "Spaghetti Plot of Observations on Each Subject Over Time")
```
As shown in the plot above, the experimental arm of the 8-week longitudinal study has higher recorded values than the control arm of the study.

# Problem 2

## Reading in and describing data

```{r, warning = FALSE, message = FALSE}
homicides = 
  read_csv("./problem_2_data/homicide-data.csv")
```
The `homicides` dataset contains `r nrow(homicides)` observations of individual homicides committed in ~50 large US cities from 2007-2017. Cities were selected based on their size and violent crime reported to the FBI in 2012. There are `r ncol(homicides)` total variables in this dataset, which include the date of the murder, victim demographics, location of the murder, and current case status.

## Data manipulations

```{r, warning = FALSE, message = FALSE}
homicides_status =
  homicides %>% 
  mutate(city = str_replace(city, "$", ", ")) %>%
  mutate(city_state = paste0(city, state)) %>% 
  group_by(city_state) 

tot_homicides = 
  homicides_status %>% 
  summarize(
    total_homicides = n()
  )

unsolved_homicides =
  homicides_status %>% 
  filter(disposition == "Closed without arrest" | disposition  == "Open/No arrest") %>% 
  summarize(
    total_unsolved = n()
  )
```

### `prop.test` for Baltimore

```{r, warning = FALSE, message = FALSE}
summary = left_join(tot_homicides, unsolved_homicides, by = "city_state") %>% 
 drop_na()
 
balt_test = summary %>% 
  filter(city_state == "Baltimore, MD") 
  
bmore_output = 
  prop.test(balt_test %>% pull(total_unsolved), balt_test %>% pull(total_homicides)) %>% 
  broom::tidy() %>% 
  select(estimate, starts_with("conf"))
```

### `prop.test` function

```{r, warning = FALSE, message = FALSE}
 tot_homicides = 
  homicides_status %>% 
  summarize(
    total_homicides = n()
  )
  
  unsolved_homicides =
  homicides_status %>% 
  filter(disposition == "Closed without arrest" | disposition  == "Open/No arrest") %>% 
  summarize(
    total_unsolved = n()
  )

summary = 
  left_join(tot_homicides, unsolved_homicides, by = "city_state") %>% 
  drop_na()
    
prop_test = 
  function(summary) {
    output =  
      prop.test(summary %>% pull(total_unsolved), summary %>% pull(total_homicides)) %>% 
      broom::tidy() 
  }

output =   
    prop.test(x, y) %>%
    broom::tidy() %>% 
    select(estimate, starts_with("conf"))


balt_test = 
  summary %>% 
  filter(city_state == "Baltimore, MD")

balt_test =
  prop_test(total_unsolved, total_homicides) %>% 
  broom::tidy() %>% 
  select(estimate, starts_with("conf"))

## create baltimore dataframe again and run function on baltimore by itself and see what happens 
```
attempt with for loop
```{r, warning = FALSE, message = FALSE}

list = as.list(summary)

output = vector("list", length = 50)

for (i in 1:50) {
  output[[i]] = prop_test(list[[i]])
}

output

prop_test(x = )
```

# Problem 3

## Generating 5000 Datasets with mu = 0

```{r, warning = FALSE, message = FALSE}
sim_t_test =
  function(n = 30, mu = 0, sigma = 5) {
 
    sim_data = tibble(
  x = rnorm(n = n, mean = mu, sd = sigma)
    )

    t.test(sim_data, mu = 0, conf.level = 0.95) %>% 
      broom::tidy() %>% 
      select(estimate, p.value)
  
  }

sim_results_df = 
  expand_grid(
  sample_size = 30,
  true_sigma = 5,
  true_mu = 0,
  iteration = 1:100
) %>% 
  mutate(
    estimate_df = map(sample_size, sim_t_test)
  ) %>% 
  unnest(estimate_df)
```

## Changing the mu = {1,2,3,4,5,6}

```{r, warning = FALSE, message = FALSE}
sim_mu_results = 
  tibble(
    mu = (0:6),
    simulations = map(.x = mu, ~rerun(100, sim_t_test(mu = .x)))
  ) %>% 
  unnest(c(simulations)) %>% 
  unnest(c(simulations))
```

## Power Plot 

```{r, warning = FALSE, message = FALSE}
sim_mu_results %>% 
  mutate(null_rejected = p.value < .05) %>% 
  group_by(mu) %>% 
  summarize(prop_rejected = mean(null_rejected)) %>% 
  ggplot(aes(x = mu, y = prop_rejected)) +
  geom_point() +
  labs(
    title = "Power by true mu",
    x = "True mu",
    y = "Power"
  )
```

Here we can see that power increases as the effect size increases; it approaches 1 as the effect size increases. The power is the probability of rejecting the null hypothesis given the null hypothesis is false. The effect size is calculated by taking the difference of mu hat from the true mu and dividing by the standard deviation. Thus, it makes sense that the bigger the true mu the larger the effect size and we see the correlation with a larger power as well.

# Plot of average estimate of μ̂ versus true value of mu

```{r, warning = FALSE, message = FALSE}
sim_mu_results %>% 
  group_by(mu) %>% 
  summarize(average_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = average_estimate)) +
  geom_point() +
  labs(
    title = "Average estimated mu among samples by true mu",
    x = "True mu",
    y = "Average estimated mu"
  )
```

# Plot of average estimate of μ̂ only in samples for which the null was rejected versus the true value of μ

```{r, warning = FALSE, message = FALSE}
sim_mu_results %>% 
  mutate(null_rejected = p.value < .05) %>% 
  filter(null_rejected == TRUE) %>% 
  group_by(mu) %>% 
  summarize(average_estimate = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = average_estimate)) +
  geom_point() +
  labs(
    title = "Average estimated mu among tests for which null is rejected, by true mu",
    x = "True mu",
    y = "Average estimated mu among tests for which null is rejected"
  )
```

# Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

We can see that for true mu 4, 5, and 6, the mu hat was a good estimate. However, for true mu of 0, the mu hat underestimated the true mu. For true mu of 1, 2, and 3, the mu hat overestimated the true mu. This indicates that the sample average of mu hat for tests where the null is rejected is generally not a good approximation of the true mu. 
This poor estimation of the true mu makes sense since we are only looking at tests where the p value is significant (a `t.test` where we conclude to reject the null). Scenarios where we reject the null would be when the estimated mu is significantly different  from the true mu (the null).
